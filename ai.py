# train_your_code_model.py
import json
import hashlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import torch
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨ (åªæ‰§è¡Œä¸€æ¬¡)
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("CUDA is available. Using GPU.")
    is_amd_gpu = False
elif torch.device("cuda:0").type == "cuda":
    device = torch.device("cuda:0")
    print("AMD GPU is available. Using GPU.")
    is_amd_gpu = True
else:
    device = torch.device("cpu")
    print("No GPU available, using CPU.")
    is_amd_gpu = False

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import os
import logging

# =============================
# é…ç½®åŒºï¼ˆæ ¹æ®ä½ çš„çŽ¯å¢ƒä¿®æ”¹ï¼‰
# =============================
REPO_ROOT = "."  # ä¿®æ”¹ä¸ºä½ çš„ä»“åº“è·¯å¾„ï¼Œå¦‚: "/path/to/your/repo"
OUTPUT_DATASET = "full_text_dataset.jsonl"
TRAINED_MODEL_OUTPUT = "qwen-coder-finetuned"  # è®­ç»ƒåŽæ¨¡åž‹ä¿å­˜è·¯å¾„
MODEL_NAME = "Qwen/Qwen2.5-Coder-1.5B"  # æ”¯æŒä¸­æ–‡/è‹±æ–‡ä»£ç 

# äºŒè¿›åˆ¶æ–‡ä»¶é»‘åå•ï¼ˆåªè¿™äº›æ‰æŽ’é™¤ï¼‰
BINARY_EXTENSIONS = {
    # å›¾åƒ
    ".jpg",
    ".jpeg",
    ".png",
    ".gif",
    ".bmp",
    ".tiff",
    ".ico",
    ".svg",
    ".webp",
    # éŸ³è§†é¢‘
    ".mp3",
    ".wav",
    ".flac",
    ".aac",
    ".ogg",
    ".mp4",
    ".avi",
    ".mov",
    ".wmv",
    ".webm",
    # åŽ‹ç¼©åŒ…
    ".zip",
    ".tar",
    ".gz",
    ".tgz",
    ".rar",
    ".7z",
    ".bz2",
    ".xz",
    ".iso",
    ".dmg",
    # å¯æ‰§è¡Œæ–‡ä»¶
    ".exe",
    ".dll",
    ".so",
    ".dylib",
    ".bin",
    ".out",
    ".class",
    ".jar",
    ".apk",
    ".ipa",
    ".pdb",
    ".o",
    ".obj",
    ".lib",
    ".a",
    # æ•°æ®åº“
    ".db",
    ".sqlite",
    ".mdb",
    ".accdb",
    # æ–‡æ¡£ï¼ˆäºŒè¿›åˆ¶ï¼‰
    ".doc",
    ".docx",
    ".xls",
    ".xlsx",
    ".ppt",
    ".pptx",
    ".pdf",
    # è®¾è®¡æ–‡ä»¶
    ".psd",
    ".ai",
    ".indd",
    ".sketch",
    ".dwg",
    ".blend",
}

# æŽ’é™¤çš„ç›®å½•ï¼ˆä¾èµ–ã€ç¼“å­˜ã€ä¸´æ—¶ï¼‰
EXCLUDE_DIRS = {
    ".git",
    "node_modules",
    "__pycache__",
    "venv",
    ".venv",
    "env",
    "dist",
    "build",
    "coverage",
    "logs",
    "log",
    "tmp",
    "temp",
    "cache",
    ".idea",
    ".vscode",
    ".pytest_cache",
    ".mypy_cache",
    ".tox",
    "examples",
    "scripts",
    "tests",
    "test",
    "testing",
    "docs",
    "doc",
    "public",
    "assets",
    "images",
    "img",
    "uploads",
    "download",
}

# æŽ’é™¤çš„æ–‡ä»¶å
EXCLUDE_FILES = {
    "train_your_code_model.py",  # æŽ’é™¤è‡ªå·±
    "package-lock.json",
    "yarn.lock",
    "pnpm-lock.yaml",
    ".gitignore",
    ".dockerignore",
    ".prettierignore",
    "README.md",
    "readme.md",
    "Readme.md",
    "LICENSE",
    "license.txt",
    "requirements.txt",
    "setup.py",
    "pyproject.toml",
    "Dockerfile",
    "docker-compose.yml",
    "Makefile",
}

# æŽ’é™¤çš„é€šé…æ¨¡å¼
EXCLUDE_PATTERNS = [
    "*.log",
    "*.tmp",
    "*~",
    ".*.sw*",
    "#*#",
    "*.pid",
    "*.min.js",
    "*.min.css",
    "*.bundle.js",
    "*.lock",
]

# æ–‡ä»¶å¤§å°é™åˆ¶
MAX_FILE_SIZE_BYTES = 1024 * 1024  # 1MB
MIN_CONTENT_LENGTH = 1  # è‡³å°‘1å­—ç¬¦

# å¤šçº¿ç¨‹
NUM_WORKERS = None  # è‡ªåŠ¨

# è¯­è¨€æ˜ å°„
EXT_TO_LANGUAGE = {
    ".py": "python",
    ".js": "javascript",
    ".jsx": "react",
    ".ts": "typescript",
    ".tsx": "typescript-react",
    ".html": "html",
    ".css": "css",
    ".sh": "shell",
    ".go": "go",
    ".rs": "rust",
    ".java": "java",
    ".cpp": "cpp",
    ".c": "c",
    ".json": "json",
    ".yaml": "yaml",
    ".yml": "yaml",
    ".md": "markdown",
    ".txt": "text",
    ".sql": "sql",
    ".tf": "terraform",
    ".ipynb": "jupyter",
    ".dockerfile": "dockerfile",
    ".xml": "xml",
    ".toml": "toml",
    ".php": "php",
}

# è®­ç»ƒå‚æ•°
MAX_SEQ_LENGTH = 1024
BATCH_SIZE = 8
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 1e-4
NUM_EPOCHS = 3
SAVE_STEPS = 50
LOG_STEPS = 10

# LoRA å‚æ•°
LORA_R = 128
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj",
]


# =============================
# åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡ä»¶
# =============================
def is_text_file(file_path: Path, sample_size: int = 1024) -> bool:
    try:
        with open(file_path, "rb") as f:
            sample = f.read(sample_size)
            if not sample:
                return False
            nontext_ratio = sum(
                1 for c in sample if c < 0x20 and c not in (9, 10, 13)
            ) / len(sample)
            return nontext_ratio < 0.3
    except Exception:
        return False


# =============================
# å¤„ç†å•ä¸ªæ–‡ä»¶
# =============================
def process_file(file_path: Path, repo_root: Path):
    try:
        ext = file_path.suffix.lower()
        if ext in BINARY_EXTENSIONS:
            return None
        if any(part.lower() in EXCLUDE_DIRS for part in file_path.parts):
            return None
        if file_path.name.lower() in EXCLUDE_FILES:
            return None
        if any(file_path.match(p) for p in EXCLUDE_PATTERNS):
            return None
        if (
            file_path.stat().st_size == 0
            or file_path.stat().st_size > MAX_FILE_SIZE_BYTES
        ):
            return None
        if not is_text_file(file_path):
            return None

        encodings = ["utf-8", "utf-8-sig", "latin1", "cp1252", "gbk"]
        content = None
        for enc in encodings:
            try:
                with open(file_path, "r", encoding=enc, errors="ignore") as f:
                    content = f.read().strip()
                break
            except Exception:
                continue
        if not content or len(content) < MIN_CONTENT_LENGTH:
            return None

        return {
            "text": content,
            "file_path": str(file_path.relative_to(repo_root)),
            "language": EXT_TO_LANGUAGE.get(ext, ext.replace(".", "") or "unknown"),
            "size": len(content),
        }
    except Exception:
        return None


# =============================
# ç”Ÿæˆæ•°æ®é›†
# =============================
def generate_dataset():
    repo_path = Path(REPO_ROOT).resolve()
    print(f"ðŸ” æ‰«æä»“åº“: {repo_path}")

    all_files = [f for f in repo_path.rglob("*") if f.is_file()]
    print(f"âœ… å‘çŽ° {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    results = []
    with ThreadPoolExecutor(
        max_workers=NUM_WORKERS or (os.cpu_count() or 1) * 2
    ) as exec:
        futures = {exec.submit(process_file, fp, repo_path): fp for fp in all_files}
        for future in as_completed(futures):
            item = future.result()
            if item:
                results.append(item)

    # åŽ»é‡
    seen = set()
    unique = []
    for item in results:
        h = hashlib.md5(item["text"].encode("utf-8")).hexdigest()
        if h not in seen:
            seen.add(h)
            unique.append(item)

    # ä¿å­˜
    with open(OUTPUT_DATASET, "w", encoding="utf-8") as f:
        for item in unique:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"ðŸŽ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆ: {OUTPUT_DATASET} (å…± {len(unique)} æ¡æ–‡æœ¬æ–‡ä»¶)")
    return unique


# =============================
# å¾®è°ƒæ¨¡åž‹
# =============================
def fine_tune():
    print("ðŸ“š åŠ è½½æ•°æ®é›†...")
    dataset = Dataset.from_json(OUTPUT_DATASET)

    print("ðŸ”§ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    def tokenize(examples):
        return tokenizer(
            examples["text"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=False
        )

    print("ðŸ”„ åˆ†è¯ä¸­...")
    tokenized_ds = dataset.map(
        tokenize, batched=True, remove_columns=["text", "file_path", "language", "size"]
    )

    print("ðŸš€ åŠ è½½ Qwen-Coder-1.5B (4-bit é‡åŒ–)...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,  # AMD GPU æŽ¨è bfloat16
    )
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,  # âœ… ä½¿ç”¨ 4-bit
        device_map="auto",
        trust_remote_code=True,
    )
    model = prepare_model_for_kbit_training(model)

    # LoRA
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=TARGET_MODULES,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=TRAINED_MODEL_OUTPUT,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        weight_decay=0.01,
        warmup_ratio=0.03,
        lr_scheduler_type="cosine",
        logging_steps=LOG_STEPS,
        save_steps=SAVE_STEPS,
        save_total_limit=2,
        bf16=True,  # âœ… æ”¹æˆ bfloat16
        fp16=False,  # âŒ å…³é—­ fp16
        max_grad_norm=0.3,
        gradient_checkpointing=True,
        report_to="none",
        eval_strategy="no",
        dataloader_pin_memory=False,
        optim="adamw_torch",
        dataloader_num_workers=os.cpu_count() or 4,
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_ds.with_format("torch"),
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
        tokenizer=tokenizer,
    )

    print("ðŸ”¥ å¼€å§‹å¾®è°ƒ...")
    trainer.train()

    print("ðŸ’¾ ä¿å­˜æ¨¡åž‹...")
    model.save_pretrained(TRAINED_MODEL_OUTPUT)
    tokenizer.save_pretrained(TRAINED_MODEL_OUTPUT)
    print(f"ðŸŽ‰ è®­ç»ƒå®Œæˆï¼æ¨¡åž‹å·²ä¿å­˜åˆ°: {TRAINED_MODEL_OUTPUT}")


# =============================
# ä¸»ç¨‹åº
# =============================
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # æ­¥éª¤1ï¼šç”Ÿæˆæ•°æ®é›†
    generate_dataset()

    # æ­¥éª¤2ï¼šå¾®è°ƒæ¨¡åž‹
    fine_tune()

    print("âœ¨ ä½ çš„ä¸“å±žä»£ç æ¨¡åž‹å·²è®­ç»ƒå®Œæˆï¼")
    print(f"ä½¿ç”¨æ–¹å¼ï¼š")
    print(f"  from transformers import AutoModelForCausalLM, AutoTokenizer")
    print(f'  model = AutoModelForCausalLM.from_pretrained("{TRAINED_MODEL_OUTPUT}")')
    print(f'  tokenizer = AutoTokenizer.from_pretrained("{TRAINED_MODEL_OUTPUT}")')
