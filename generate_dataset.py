# train_your_code_model.py
import json
import hashlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"

import os
import logging

# =============================
# é…ç½®åŒºï¼ˆæ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹ï¼‰
# =============================
REPO_ROOT = "."
OUTPUT_DATASET = "full_text_dataset.json"
TRAINED_MODEL_OUTPUT = "qwen-coder-finetuned"
MODEL_NAME = "Qwen/Qwen2.5-Coder-1.5B"

# äºŒè¿›åˆ¶æ–‡ä»¶é»‘åå•ï¼ˆåªè¿™äº›æ‰æ’é™¤ï¼‰
BINARY_EXTENSIONS = {
    ".jpg",
    ".jpeg",
    ".png",
    ".gif",
    ".bmp",
    ".tiff",
    ".ico",
    ".svg",
    ".webp",
    ".mp3",
    ".wav",
    ".flac",
    ".aac",
    ".ogg",
    ".mp4",
    ".avi",
    ".mov",
    ".wmv",
    ".webm",
    ".zip",
    ".tar",
    ".gz",
    ".tgz",
    ".rar",
    ".7z",
    ".bz2",
    ".xz",
    ".iso",
    ".dmg",
    ".exe",
    ".dll",
    ".so",
    ".dylib",
    ".bin",
    ".out",
    ".class",
    ".jar",
    ".apk",
    ".ipa",
    ".pdb",
    ".o",
    ".obj",
    ".lib",
    ".a",
    ".db",
    ".sqlite",
    ".mdb",
    ".accdb",
    ".doc",
    ".docx",
    ".xls",
    ".xlsx",
    ".ppt",
    ".pptx",
    ".pdf",
    ".psd",
    ".ai",
    ".indd",
    ".sketch",
    ".dwg",
    ".blend",
}

# æ’é™¤çš„ç›®å½•ï¼ˆä¾èµ–ã€ç¼“å­˜ã€ä¸´æ—¶ï¼‰
EXCLUDE_DIRS = {
    ".git",
    "node_modules",
    "__pycache__",
    "venv",
    ".venv",
    "env",
    "dist",
    "build",
    "coverage",
    "logs",
    "log",
    "tmp",
    "temp",
    "cache",
    ".idea",
    ".vscode",
    ".pytest_cache",
    ".mypy_cache",
    ".tox",
    "examples",
    "scripts",
    "tests",
    "test",
    "testing",
    "docs",
    "doc",
    "public",
    "assets",
    "images",
    "img",
    "uploads",
    "download",
}

# æ’é™¤çš„æ–‡ä»¶å
EXCLUDE_FILES = {
    "ai.py",
    "generate_dataset.py",
    "clone_repos.sh",
    "train_your_code_model.py",
    "package-lock.json",
    "yarn.lock",
    "pnpm-lock.yaml",
    ".gitignore",
    ".dockerignore",
    ".prettierignore",
    "README.md",
    "readme.md",
    "Readme.md",
    "LICENSE",
    "license.txt",
    "requirements.txt",
    "setup.py",
    "pyproject.toml",
    "Dockerfile",
    "docker-compose.yml",
    "Makefile",
}

# æ’é™¤çš„é€šé…æ¨¡å¼
EXCLUDE_PATTERNS = [
    "*.log",
    "*.tmp",
    "*~",
    ".*.sw*",
    "#*#",
    "*.pid",
    "*.min.js",
    "*.min.css",
    "*.bundle.js",
    "*.lock",
]


# å¤šçº¿ç¨‹
NUM_WORKERS = 8

# è¯­è¨€æ˜ å°„
EXT_TO_LANGUAGE = {
    ".py": "python",
    ".js": "javascript",
    ".jsx": "react",
    ".ts": "typescript",
    ".tsx": "typescript-react",
    ".html": "html",
    ".css": "css",
    ".sh": "shell",
    ".go": "go",
    ".rs": "rust",
    ".java": "java",
    ".cpp": "cpp",
    ".c": "c",
    ".json": "json",
    ".yaml": "yaml",
    ".yml": "yaml",
    ".md": "markdown",
    ".txt": "text",
    ".sql": "sql",
    ".tf": "terraform",
    ".ipynb": "jupyter",
    ".dockerfile": "dockerfile",
    ".xml": "xml",
    ".toml": "toml",
    ".php": "php",
}

# è®­ç»ƒå‚æ•°
MAX_SEQ_LENGTH = 1024
BATCH_SIZE = 8
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 1e-4
NUM_EPOCHS = 3
SAVE_STEPS = 50
LOG_STEPS = 10

# LoRA å‚æ•°
LORA_R = 128
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj",
]


# =============================
# åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡ä»¶
# =============================
def is_text_file(file_path: Path, sample_size: int = 1024) -> bool:
    """
    Checks if a file is a text file by sampling its content.

    Args:
        - `file_path`: Path to the file.
        - `sample_size`: Number of bytes to sample from the file.

    Returns:
        True if the file is likely a text file, False otherwise.
    """
    try:
        with open(file_path, "rb") as f:
            sample = f.read(sample_size)
            if not sample:
                return False
            nontext_ratio = sum(
                1 for c in sample if c < 0x20 and c not in (9, 10, 13)
            ) / len(sample)
            return nontext_ratio < 0.3
    except Exception:
        return False


# =============================
# å¤„ç†å•ä¸ªæ–‡ä»¶
# =============================
# =============================
# å¤„ç†å•ä¸ªæ–‡ä»¶
# =============================


def get_file_language(file_path: Path, content: str = None):
    """
    æ™ºèƒ½æ£€æµ‹æ–‡ä»¶è¯­è¨€ç±»å‹

    Args:
        file_path (Path): æ–‡ä»¶è·¯å¾„
        content (str): æ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼‰

    Returns:
        str: æ£€æµ‹åˆ°çš„è¯­è¨€ç±»å‹
    """
    ext = file_path.suffix.lower()
    filename = file_path.name.lower()

    # é¦–å…ˆå°è¯•æ‰©å±•ååŒ¹é…
    if ext in EXT_TO_LANGUAGE:
        return EXT_TO_LANGUAGE[ext]

    # ç‰¹æ®Šæ–‡ä»¶åæ£€æµ‹
    special_files = {
        "license": "license",
        "readme": "markdown",
        "changelog": "markdown",
        "dockerfile": "dockerfile",
        "makefile": "makefile",
        "jenkinsfile": "groovy",
        "vagrantfile": "ruby",
        "gemfile": "ruby",
        "requirements.txt": "text",
        "package.json": "json",
        "composer.json": "json",
        "cargo.toml": "toml",
        "pyproject.toml": "toml",
        ".gitignore": "gitignore",
        ".gitattributes": "gitattributes",
        ".env": "env",
        ".dockerignore": "dockerignore",
    }

    for pattern, lang in special_files.items():
        if pattern in filename:
            return lang

    # åŸºäºæ–‡ä»¶å†…å®¹çš„æ£€æµ‹ï¼ˆå¦‚æœæä¾›äº†å†…å®¹ï¼‰
    if content:
        content_lower = content.lower().strip()

        # LICENSEæ–‡ä»¶æ£€æµ‹
        if any(
            keyword in content_lower
            for keyword in ["license", "copyright", "permission is hereby granted"]
        ):
            return "license"

        # Shellè„šæœ¬æ£€æµ‹
        if content.startswith("#!/bin/bash") or content.startswith("#!/bin/sh"):
            return "shell"

        # Pythonè„šæœ¬æ£€æµ‹
        if content.startswith("#!/usr/bin/env python") or content.startswith(
            "#!/usr/bin/python"
        ):
            return "python"

        # HTMLæ£€æµ‹
        if content_lower.startswith("<!doctype html") or "<html" in content_lower:
            return "html"

        # XMLæ£€æµ‹
        if content.startswith("<?xml"):
            return "xml"

        # JSONæ£€æµ‹
        if (content.startswith("{") and content.endswith("}")) or (
            content.startswith("[") and content.endswith("]")
        ):
            try:
                import json

                json.loads(content)
                return "json"
            except:
                pass

    # å¦‚æœéƒ½æ— æ³•è¯†åˆ«ï¼Œè¿”å›æ–‡ä»¶æ‰©å±•åæˆ–unknown
    return ext.replace(".", "") if ext else "text"


def process_file(file_path: Path, repo_root: Path):
    """
    Process a single file to extract content and metadata.

    Args:
        file_path (Path): Path to the file.
        repo_root (Path): Path to the repository root.

    Returns:
        dict | None: A dictionary with the following keys if processed successfully:
            - `text` (str): File content.
            - `file_path` (str): Relative path from repo root.
            - `language` (str): Detected language based on extension.
            - `size` (int): Length of the text content.
        Returns None if the file is excluded or cannot be processed.
    """
    try:
        ext = file_path.suffix.lower()
        if ext in BINARY_EXTENSIONS:
            return None
        if any(part.lower() in EXCLUDE_DIRS for part in file_path.parts):
            return None
        if file_path.name.lower() in EXCLUDE_FILES:
            return None
        if any(file_path.match(p) for p in EXCLUDE_PATTERNS):
            return None
        if file_path.stat().st_size == 0:
            return None
        if not is_text_file(file_path):
            return None

        encodings = ["utf-8", "utf-8-sig", "latin1", "cp1252", "gbk"]
        content = None
        for enc in encodings:
            try:
                with open(file_path, "r", encoding=enc, errors="ignore") as f:
                    content = f.read().strip()
                break
            except Exception:
                continue

        return {
            "text": content,
            "file_path": str(file_path.relative_to(repo_root)),
            "language": get_file_language(file_path, content),
            "size": len(content),
        }
    except Exception:
        return None


def create_alpaca_entry(item):
    """
    Create an Alpaca format entry from processed file item.

    Args:
        item (dict): Processed file item with text, file_path, language, size

    Returns:
        dict: Alpaca format entry
    """
    file_path = item["file_path"]
    language = item["language"]
    content = item["text"]

    # æ ¹æ®æ–‡ä»¶ç±»å‹ç”Ÿæˆä¸åŒçš„instructionå’Œsystem
    if language in ["python", "py"]:
        instruction = "è¯·è¯¦ç»†åˆ†æè¿™ä¸ªPythonä»£ç æ–‡ä»¶çš„åŠŸèƒ½ã€ç»“æ„å’Œå®ç°é€»è¾‘"
        system = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonä»£ç åˆ†æå¸ˆï¼Œèƒ½å¤Ÿæ·±å…¥ç†è§£ä»£ç çš„åŠŸèƒ½ã€æ¶æ„è®¾è®¡å’Œå®ç°ç»†èŠ‚ã€‚ä½ ä¼šä»ä»£ç ç»“æ„ã€ç®—æ³•é€»è¾‘ã€è®¾è®¡æ¨¡å¼ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œå…¨é¢åˆ†æã€‚"
    elif language in ["javascript", "js", "ts", "typescript"]:
        instruction = "è¯·åˆ†æè¿™ä¸ªJavaScript/TypeScriptä»£ç çš„åŠŸèƒ½å®ç°å’Œè®¾è®¡æ¨¡å¼"
        system = "ä½ æ˜¯ä¸€ä¸ªå‰ç«¯å¼€å‘ä¸“å®¶ï¼Œæ“…é•¿åˆ†æJavaScriptå’ŒTypeScriptä»£ç çš„è®¾è®¡æ¨¡å¼ã€æœ€ä½³å®è·µå’Œæ€§èƒ½ä¼˜åŒ–ã€‚"
    elif language in ["rs"]:
        instruction = "è¯·åˆ†æè¿™ä¸ªRustä»£ç çš„åŠŸèƒ½å®ç°å’Œè®¾è®¡æ¨¡å¼"
        system = (
            "ä½ æ˜¯ä¸€ä¸ªRustå¼€å‘ä¸“å®¶ï¼Œæ“…é•¿åˆ†æRustä»£ç çš„è®¾è®¡æ¨¡å¼ã€æœ€ä½³å®è·µå’Œæ€§èƒ½ä¼˜åŒ–ã€‚"
        )
    elif language in ["java"]:
        instruction = "è¯·è§£æè¿™ä¸ªJavaä»£ç çš„é¢å‘å¯¹è±¡è®¾è®¡å’ŒåŠŸèƒ½å®ç°"
        system = (
            "ä½ æ˜¯ä¸€ä¸ªJavaå¼€å‘ä¸“å®¶ï¼Œç²¾é€šJavaçš„é¢å‘å¯¹è±¡ç¼–ç¨‹ã€è®¾è®¡æ¨¡å¼å’Œä¼ä¸šçº§åº”ç”¨å¼€å‘ã€‚"
        )
    elif language in ["cpp", "c++", "c"]:
        instruction = "è¯·åˆ†æè¿™ä¸ªC/C++ä»£ç çš„ç®—æ³•å®ç°å’Œç³»ç»Ÿè®¾è®¡"
        system = "ä½ æ˜¯ä¸€ä¸ªç³»ç»Ÿçº§ç¼–ç¨‹ä¸“å®¶ï¼Œç²¾é€šC/C++çš„å†…å­˜ç®¡ç†ã€ç®—æ³•ä¼˜åŒ–ã€æ•°æ®ç»“æ„å’Œç³»ç»Ÿè®¾è®¡ã€‚"
    elif language in ["html"]:
        instruction = "è¯·åˆ†æè¿™ä¸ªHTMLæ–‡ä»¶çš„ç»“æ„ã€è¯­ä¹‰å’Œè®¾è®¡è§„èŒƒ"
        system = "ä½ æ˜¯ä¸€ä¸ªå‰ç«¯å¼€å‘ä¸“å®¶ï¼Œç²¾é€šHTMLè¯­ä¹‰åŒ–ã€Webæ ‡å‡†å’Œç”¨æˆ·ä½“éªŒè®¾è®¡ã€‚"
    elif language in ["css"]:
        instruction = "è¯·è§£é‡Šè¿™ä¸ªCSSæ ·å¼æ–‡ä»¶çš„è®¾è®¡æ€è·¯å’Œå¸ƒå±€å®ç°"
        system = "ä½ æ˜¯ä¸€ä¸ªUI/UXè®¾è®¡å¸ˆï¼Œç²¾é€šCSSå¸ƒå±€ã€å“åº”å¼è®¾è®¡ã€åŠ¨ç”»æ•ˆæœå’Œç°ä»£CSSç‰¹æ€§ã€‚"
    elif language in ["markdown", "md"]:
        instruction = "è¯·æ€»ç»“è¿™ä¸ªMarkdownæ–‡æ¡£çš„ä¸»è¦å†…å®¹ã€ç»“æ„å’Œè¦ç‚¹"
        system = (
            "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ä¸“å®¶ï¼Œæ“…é•¿æå–æ–‡æ¡£æ ¸å¿ƒä¿¡æ¯ã€åˆ†ææ–‡æ¡£ç»“æ„å’Œæ€»ç»“å…³é”®è¦ç‚¹ã€‚"
        )
    elif language in ["json"]:
        instruction = "è¯·è§£é‡Šè¿™ä¸ªJSONæ–‡ä»¶çš„æ•°æ®ç»“æ„ã€é…ç½®é¡¹å’Œä½¿ç”¨åœºæ™¯"
        system = "ä½ æ˜¯ä¸€ä¸ªç³»ç»Ÿé…ç½®ä¸“å®¶ï¼Œèƒ½å¤Ÿè§£æå„ç§é…ç½®æ–‡ä»¶æ ¼å¼ï¼Œç†è§£é…ç½®é¡¹çš„ä½œç”¨å’Œæœ€ä½³å®è·µã€‚"
    elif language in ["yaml", "yml"]:
        instruction = "è¯·åˆ†æè¿™ä¸ªYAMLé…ç½®æ–‡ä»¶çš„ç»“æ„ã€é…ç½®é¡¹å’Œåº”ç”¨åœºæ™¯"
        system = "ä½ æ˜¯ä¸€ä¸ªDevOpså·¥ç¨‹å¸ˆï¼Œç²¾é€šå„ç§é…ç½®æ–‡ä»¶æ ¼å¼ã€éƒ¨ç½²é…ç½®å’Œè‡ªåŠ¨åŒ–è¿ç»´ã€‚"
    elif language == "license":
        instruction = "è¯·åˆ†æè¿™ä¸ªå¼€æºè®¸å¯è¯æ–‡ä»¶çš„å†…å®¹å’Œæ³•å¾‹æ¡æ¬¾"
        system = "ä½ æ˜¯ä¸€ä¸ªå¼€æºè®¸å¯è¯ä¸“å®¶ï¼Œç†Ÿæ‚‰å„ç§å¼€æºè®¸å¯è¯çš„æ¡æ¬¾ã€é™åˆ¶å’Œä½¿ç”¨åœºæ™¯ã€‚"
    elif language == "dockerfile":
        instruction = "è¯·åˆ†æè¿™ä¸ªDockerfileçš„æ„å»ºé€»è¾‘å’Œå®¹å™¨é…ç½®"
        system = "ä½ æ˜¯ä¸€ä¸ªDevOpså·¥ç¨‹å¸ˆï¼Œç²¾é€šDockerå®¹å™¨æŠ€æœ¯ã€é•œåƒæ„å»ºå’Œéƒ¨ç½²ä¼˜åŒ–ã€‚"
    elif language in ["shell", "bash"]:
        instruction = "è¯·åˆ†æè¿™ä¸ªShellè„šæœ¬çš„åŠŸèƒ½é€»è¾‘å’Œç³»ç»Ÿæ“ä½œ"
        system = "ä½ æ˜¯ä¸€ä¸ªç³»ç»Ÿç®¡ç†å‘˜ï¼Œç²¾é€šShellè„šæœ¬ç¼–ç¨‹ã€ç³»ç»Ÿè¿ç»´å’Œè‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚"
    elif language in ["xml"]:
        instruction = "è¯·åˆ†æè¿™ä¸ªXMLæ–‡ä»¶çš„ç»“æ„å’Œæ•°æ®ç»„ç»‡æ–¹å¼"
        system = "ä½ æ˜¯ä¸€ä¸ªæ•°æ®ç»“æ„ä¸“å®¶ï¼Œç†Ÿæ‚‰XMLæ ¼å¼ã€æ•°æ®å»ºæ¨¡å’Œç»“æ„åŒ–æ•°æ®å¤„ç†ã€‚"
    elif language in ["gitignore"]:
        instruction = "è¯·è§£é‡Šè¿™ä¸ªGitå¿½ç•¥æ–‡ä»¶çš„é…ç½®è§„åˆ™å’Œä½œç”¨"
        system = "ä½ æ˜¯ä¸€ä¸ªç‰ˆæœ¬æ§åˆ¶ä¸“å®¶ï¼Œç²¾é€šGitå·¥ä½œæµã€ä»£ç ç®¡ç†å’Œé¡¹ç›®é…ç½®ã€‚"
    else:
        instruction = f"è¯·åˆ†æè¿™ä¸ª{language}æ–‡ä»¶çš„å†…å®¹ç»“æ„å’Œä¸»è¦åŠŸèƒ½"
        system = f"ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„{language}å¼€å‘ä¸“å®¶ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æä»£ç ç»“æ„ã€å®ç°é€»è¾‘å’ŒæŠ€æœ¯ç‰¹ç‚¹ã€‚"

    content_preview = content
    size_note = f"æ–‡ä»¶å¤§å°: {item['size']} å­—ç¬¦"

    # inputåŒ…å«æ–‡ä»¶çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå†…å®¹
    input_text = f"""æ–‡ä»¶: {file_path}
è¯­è¨€: {language}
{size_note}

ä»£ç å†…å®¹:
```{language}
{content_preview}
```"""

    # ç”Ÿæˆæ›´çœŸå®çš„outputï¼ŒåŸºäºå®é™…ä»£ç å†…å®¹
    output = generate_realistic_analysis(content, language, file_path, item["size"])

    return {
        "instruction": instruction,
        "input": input_text,
        "output": output,
        "system": system,
        "history": [],
    }


def generate_realistic_analysis(content, language, file_path, size):
    """
    ç”Ÿæˆæ›´çœŸå®çš„ä»£ç åˆ†æå›ç­”
    """
    # ç®€å•çš„ä»£ç åˆ†æé€»è¾‘
    lines = content.split("\n")
    non_empty_lines = [line for line in lines if line.strip()]

    # æ£€æµ‹ä¸€äº›åŸºæœ¬ç‰¹å¾
    has_functions = any(
        "def " in line or "function " in line or "func " in line for line in lines
    )
    has_classes = any("class " in line for line in lines)
    has_imports = any(
        line.strip().startswith(("import ", "from ", "#include", "require(", "const "))
        for line in lines
    )
    has_comments = any(
        line.strip().startswith(("#", "//", "/*", "<!--")) for line in lines
    )

    analysis = f"è¿™æ˜¯ä¸€ä¸ª{language}æ–‡ä»¶ï¼Œä½äº `{file_path}`ï¼ŒåŒ…å«{size}ä¸ªå­—ç¬¦ï¼Œå…±{len(lines)}è¡Œä»£ç ã€‚\n\n"

    # ç»“æ„åˆ†æ
    analysis += "**ä»£ç ç»“æ„åˆ†æ:**\n"
    if has_imports:
        analysis += "- åŒ…å«æ¨¡å—å¯¼å…¥/å¼•ç”¨è¯­å¥ï¼Œè¯´æ˜ä»£ç ä¾èµ–å…¶ä»–æ¨¡å—æˆ–åº“\n"
    if has_classes:
        analysis += "- å®šä¹‰äº†ç±»ç»“æ„ï¼Œé‡‡ç”¨é¢å‘å¯¹è±¡ç¼–ç¨‹æ–¹å¼\n"
    if has_functions:
        analysis += "- åŒ…å«å‡½æ•°å®šä¹‰ï¼Œä»£ç æ¨¡å—åŒ–ç¨‹åº¦è¾ƒå¥½\n"
    if has_comments:
        analysis += "- æœ‰æ³¨é‡Šè¯´æ˜ï¼Œä»£ç å¯è¯»æ€§è¾ƒå¥½\n"

    # åŠŸèƒ½åˆ†æ
    analysis += "\n**ä¸»è¦åŠŸèƒ½ç‰¹ç‚¹:**\n"

    if language in ["python", "py"]:
        if "def " in content:
            func_count = content.count("def ")
            analysis += f"- å®šä¹‰äº†{func_count}ä¸ªå‡½æ•°ï¼Œå®ç°ç‰¹å®šçš„åŠŸèƒ½é€»è¾‘\n"
        if "class " in content:
            class_count = content.count("class ")
            analysis += f"- åŒ…å«{class_count}ä¸ªç±»å®šä¹‰ï¼Œé‡‡ç”¨é¢å‘å¯¹è±¡è®¾è®¡\n"
        if "import " in content or "from " in content:
            analysis += "- ä½¿ç”¨äº†å¤–éƒ¨åº“ä¾èµ–ï¼Œæ‰©å±•äº†åŠŸèƒ½å®ç°\n"

    elif language in ["javascript", "js"]:
        if "function" in content or "=>" in content:
            analysis += "- åŒ…å«JavaScriptå‡½æ•°å®šä¹‰ï¼Œå®ç°äº¤äº’é€»è¾‘\n"
        if "const " in content or "let " in content:
            analysis += "- ä½¿ç”¨ç°ä»£JavaScriptè¯­æ³•ï¼Œä»£ç è§„èŒƒæ€§è¾ƒå¥½\n"
        if "async" in content or "await" in content:
            analysis += "- é‡‡ç”¨å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼ï¼Œå¤„ç†å¼‚æ­¥æ“ä½œ\n"

    elif language in ["html"]:
        if "<script" in content:
            analysis += "- åŒ…å«JavaScriptè„šæœ¬ï¼Œå…·æœ‰äº¤äº’åŠŸèƒ½\n"
        if "<style" in content or "css" in content:
            analysis += "- åŒ…å«æ ·å¼å®šä¹‰ï¼Œæ³¨é‡é¡µé¢å¤–è§‚è®¾è®¡\n"
        if "<!DOCTYPE" in content:
            analysis += "- ä½¿ç”¨æ ‡å‡†HTML5æ–‡æ¡£ç±»å‹å£°æ˜\n"

    elif language in ["css"]:
        selector_count = content.count("{")
        analysis += f"- åŒ…å«çº¦{selector_count}ä¸ªCSSè§„åˆ™ï¼Œå®šä¹‰é¡µé¢æ ·å¼\n"
        if "@media" in content:
            analysis += "- ä½¿ç”¨åª’ä½“æŸ¥è¯¢ï¼Œæ”¯æŒå“åº”å¼è®¾è®¡\n"
        if "animation" in content or "transition" in content:
            analysis += "- åŒ…å«åŠ¨ç”»æ•ˆæœï¼Œæå‡ç”¨æˆ·ä½“éªŒ\n"

    # ä»£ç è´¨é‡è¯„ä¼°
    analysis += "\n**ä»£ç è´¨é‡è¯„ä¼°:**\n"
    comment_ratio = sum(
        1 for line in lines if line.strip().startswith(("#", "//", "/*", "<!--"))
    ) / max(len(non_empty_lines), 1)

    if comment_ratio > 0.1:
        analysis += "- æ³¨é‡Šè¾ƒä¸ºå……åˆ†ï¼Œä»£ç å¯ç»´æŠ¤æ€§å¥½\n"
    elif comment_ratio > 0.05:
        analysis += "- æœ‰é€‚é‡æ³¨é‡Šï¼ŒåŸºæœ¬æ»¡è¶³å¯è¯»æ€§è¦æ±‚\n"
    else:
        analysis += "- æ³¨é‡Šç›¸å¯¹è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ å¿…è¦çš„è¯´æ˜\n"

    if len(non_empty_lines) < 50:
        analysis += "- ä»£ç è¾ƒä¸ºç®€æ´ï¼Œç»“æ„æ¸…æ™°\n"
    elif len(non_empty_lines) < 200:
        analysis += "- ä»£ç è§„æ¨¡é€‚ä¸­ï¼Œé€»è¾‘ç›¸å¯¹å®Œæ•´\n"
    else:
        analysis += "- ä»£ç è§„æ¨¡è¾ƒå¤§ï¼ŒåŠŸèƒ½ç›¸å¯¹å¤æ‚\n"

    analysis += f"\nè¿™ä¸ª{language}æ–‡ä»¶å±•ç°äº†è‰¯å¥½çš„ç¼–ç¨‹å®è·µï¼Œå»ºè®®ç»“åˆå…·ä½“ä¸šåŠ¡éœ€æ±‚è¿›ä¸€æ­¥ä¼˜åŒ–å’Œå®Œå–„ã€‚"

    return analysis


def generate_dataset():
    """
    Generate an Alpaca format dataset from repository files.
    Reads ALL files except those in blacklist, without limiting count or length.

    Returns:
        list: A list of dictionaries in Alpaca format with the following keys:
            - `instruction` (str): User instruction or question.
            - `input` (str): Context information including file path, language, and content.
            - `output` (str): Model response analyzing the code.
            - `system` (str): System prompt or role setting.
            - `history` (list): Historical dialogue, empty for new conversations.
    """
    repo_path = Path(REPO_ROOT).resolve()
    print(f"ğŸ” æ‰«æä»“åº“: {repo_path}")

    all_files = [f for f in repo_path.rglob("*") if f.is_file()]
    print(f"âœ… å‘ç° {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    # å¤„ç†æ–‡ä»¶å†…å®¹ - ç§»é™¤æ‰€æœ‰é™åˆ¶ï¼Œåªä¿ç•™é»‘åå•è¿‡æ»¤
    results = []
    processed_count = 0
    skipped_count = 0

    with ThreadPoolExecutor(
        max_workers=NUM_WORKERS or (os.cpu_count() or 1) * 2
    ) as exec:
        futures = {
            exec.submit(process_file_unlimited, fp, repo_path): fp for fp in all_files
        }

        for future in as_completed(futures):
            item = future.result()
            if item:
                results.append(item)
                processed_count += 1
                if processed_count % 100 == 0:  # æ¯å¤„ç†100ä¸ªæ–‡ä»¶æ‰“å°ä¸€æ¬¡è¿›åº¦
                    print(f"ğŸ“ å·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶...")
            else:
                skipped_count += 1

    print(f"ğŸ“Š å¤„ç†å®Œæˆ: æˆåŠŸ {processed_count} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ª")

    # ä¸è¿›è¡Œå»é‡ï¼Œä¿ç•™æ‰€æœ‰æ–‡ä»¶ï¼ˆé™¤éç”¨æˆ·ç‰¹åˆ«éœ€è¦å»é‡ï¼‰
    # å¦‚æœéœ€è¦å»é‡ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢æ³¨é‡Š
    """
    seen = set()
    unique = []
    for item in results:
        h = hashlib.md5(item["text"].encode("utf-8")).hexdigest()
        if h not in seen:
            seen.add(h)
            unique.append(item)
    results = unique
    """

    # è½¬æ¢ä¸ºAlpacaæ ¼å¼
    print(f"ğŸ”„ è½¬æ¢ä¸ºAlpacaæ ¼å¼...")
    alpaca_dataset = []
    conversion_errors = 0

    for i, item in enumerate(results):
        try:
            alpaca_entry = create_alpaca_entry(item)
            alpaca_dataset.append(alpaca_entry)

            if (i + 1) % 500 == 0:  # æ¯è½¬æ¢500ä¸ªæ¡ç›®æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"ğŸ”„ å·²è½¬æ¢ {i + 1}/{len(results)} ä¸ªæ¡ç›®...")

        except Exception as e:
            conversion_errors += 1
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {item.get('file_path', 'unknown')} æ—¶å‡ºé”™: {e}")
            continue

    if conversion_errors > 0:
        print(f"âš ï¸  è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç° {conversion_errors} ä¸ªé”™è¯¯")

    # ä¿å­˜ä¸ºæ ‡å‡†JSONæ ¼å¼ï¼ˆä¸æ˜¯JSONLï¼‰
    output_file = OUTPUT_DATASET.replace(".jsonl", "_alpaca.json")

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®é›†åˆ° {output_file}...")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(alpaca_dataset, f, ensure_ascii=False, indent=2)

    print(f"ğŸ‰ Alpacaæ ¼å¼æ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ€»æ•°: {len(alpaca_dataset)}")

    # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    print_dataset_stats(alpaca_dataset)

    return alpaca_dataset


def process_file_unlimited(file_path: Path, repo_root: Path):
    """
    Process a single file without size or content limitations.
    Only applies blacklist filtering.

    Args:
        file_path (Path): Path to the file.
        repo_root (Path): Path to the repository root.

    Returns:
        dict | None: A dictionary with file content and metadata if processed successfully.
    """
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ‰©å±•åé»‘åå•
        ext = file_path.suffix.lower()
        if ext in BINARY_EXTENSIONS:
            return None

        # 2. æ£€æŸ¥ç›®å½•é»‘åå•
        if any(part.lower() in EXCLUDE_DIRS for part in file_path.parts):
            return None

        # 3. æ£€æŸ¥æ–‡ä»¶åé»‘åå•
        if file_path.name.lower() in EXCLUDE_FILES:
            return None

        # 4. æ£€æŸ¥é€šé…ç¬¦æ¨¡å¼é»‘åå•
        if any(file_path.match(p) for p in EXCLUDE_PATTERNS):
            return None

        # 5. ç§»é™¤ç©ºæ–‡ä»¶æ£€æŸ¥é™åˆ¶ï¼Œå…è®¸å¤„ç†ç©ºæ–‡ä»¶
        # if file_path.stat().st_size == 0:
        #     return None

        # 6. ç§»é™¤æ–‡æœ¬æ–‡ä»¶æ£€æŸ¥ï¼Œå…è®¸å¤„ç†æ‰€æœ‰éäºŒè¿›åˆ¶æ–‡ä»¶
        # ä½†ä¿ç•™åŸºæœ¬çš„æ–‡æœ¬æ£€æµ‹ä»¥é¿å…å¤„ç†çœŸæ­£çš„äºŒè¿›åˆ¶æ–‡ä»¶
        if not is_text_file_permissive(file_path):
            return None

        # 7. è¯»å–æ–‡ä»¶å†…å®¹ï¼Œç§»é™¤é•¿åº¦é™åˆ¶
        encodings = ["utf-8", "utf-8-sig", "latin1", "cp1252", "gbk", "iso-8859-1"]
        content = None
        encoding_used = None

        for enc in encodings:
            try:
                with open(file_path, "r", encoding=enc, errors="ignore") as f:
                    content = f.read()  # ç§»é™¤ .strip()ï¼Œä¿ç•™åŸå§‹æ ¼å¼
                encoding_used = enc
                break
            except Exception as e:
                continue

        if content is None:
            return None

        return {
            "text": content,
            "file_path": str(file_path.relative_to(repo_root)),
            "language": get_file_language(file_path, content),
            "size": len(content),
            "encoding": encoding_used,
            "lines": len(content.split("\n")) if content else 0,
        }

    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def is_text_file_permissive(file_path: Path, sample_size: int = 2048) -> bool:
    """
    More permissive text file detection.

    Args:
        file_path: Path to the file.
        sample_size: Number of bytes to sample from the file.

    Returns:
        True if the file is likely a text file, False otherwise.
    """
    try:
        # å…ˆæ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¯¹äºå¾ˆå¤§çš„æ–‡ä»¶åªæ£€æŸ¥å¼€å¤´éƒ¨åˆ†
        file_size = file_path.stat().st_size

        # ç©ºæ–‡ä»¶ä¹Ÿè®¤ä¸ºæ˜¯æ–‡æœ¬æ–‡ä»¶
        if file_size == 0:
            return True

        # å¯¹äºéå¸¸å¤§çš„æ–‡ä»¶ï¼Œå¢åŠ é‡‡æ ·å¤§å°
        actual_sample_size = min(sample_size, file_size)

        with open(file_path, "rb") as f:
            sample = f.read(actual_sample_size)

            if not sample:
                return True  # ç©ºæ–‡ä»¶è®¤ä¸ºæ˜¯æ–‡æœ¬æ–‡ä»¶

            # æ›´å®½æ¾çš„æ–‡æœ¬æ£€æµ‹ï¼šå…è®¸æ›´é«˜çš„éæ–‡æœ¬å­—ç¬¦æ¯”ä¾‹
            nontext_ratio = sum(
                1 for c in sample if c < 0x20 and c not in (9, 10, 13)
            ) / len(sample)

            # æé«˜é˜ˆå€¼åˆ°0.5ï¼Œå…è®¸æ›´å¤šçš„äºŒè¿›åˆ¶å†…å®¹
            return nontext_ratio < 0.5

    except Exception:
        # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œä¿å®ˆåœ°è®¤ä¸ºæ˜¯æ–‡æœ¬æ–‡ä»¶
        return True


def print_dataset_stats(alpaca_dataset):
    """
    Print detailed statistics about the generated dataset.

    Args:
        alpaca_dataset: List of Alpaca format entries
    """
    if not alpaca_dataset:
        print("ğŸ“Š æ•°æ®é›†ä¸ºç©º")
        return

    print("\n" + "=" * 50)
    print("ğŸ“Š æ•°æ®é›†è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 50)

    # è¯­è¨€ç±»å‹ç»Ÿè®¡
    language_stats = {}
    total_size = 0
    total_lines = 0

    for entry in alpaca_dataset:
        input_text = entry["input"]

        # æå–è¯­è¨€ä¿¡æ¯
        if "è¯­è¨€: " in input_text:
            lang = input_text.split("è¯­è¨€: ")[1].split("\n")[0].strip()
            language_stats[lang] = language_stats.get(lang, 0) + 1

        # æå–å¤§å°ä¿¡æ¯
        if "æ–‡ä»¶å¤§å°: " in input_text:
            size_str = input_text.split("æ–‡ä»¶å¤§å°: ")[1].split(" ")[0]
            try:
                size = int(size_str)
                total_size += size
            except:
                pass

    # æ‰“å°è¯­è¨€åˆ†å¸ƒ
    print(f"ğŸˆ´ è¯­è¨€ç±»å‹åˆ†å¸ƒ (å…± {len(language_stats)} ç§è¯­è¨€):")
    for lang, count in sorted(language_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(alpaca_dataset)) * 100
        print(f"   {lang:15} {count:6} ä¸ªæ–‡ä»¶ ({percentage:5.1f}%)")

    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»è®­ç»ƒæ ·æœ¬æ•°: {len(alpaca_dataset):,}")
    print(f"   æ€»å­—ç¬¦æ•°:     {total_size:,}")
    print(
        f"   å¹³å‡æ–‡ä»¶å¤§å°: {total_size // len(alpaca_dataset) if len(alpaca_dataset) > 0 else 0:,} å­—ç¬¦"
    )

    # æ–‡ä»¶å¤§å°åˆ†å¸ƒ
    sizes = []
    for entry in alpaca_dataset:
        input_text = entry["input"]
        if "æ–‡ä»¶å¤§å°: " in input_text:
            size_str = input_text.split("æ–‡ä»¶å¤§å°: ")[1].split(" ")[0]
            try:
                size = int(size_str)
                sizes.append(size)
            except:
                pass

    if sizes:
        sizes.sort()
        print(f"\nğŸ“ æ–‡ä»¶å¤§å°åˆ†å¸ƒ:")
        print(f"   æœ€å°æ–‡ä»¶: {min(sizes):,} å­—ç¬¦")
        print(f"   æœ€å¤§æ–‡ä»¶: {max(sizes):,} å­—ç¬¦")
        print(f"   ä¸­ä½æ•°:   {sizes[len(sizes)//2]:,} å­—ç¬¦")

        # å¤§å°åŒºé—´åˆ†å¸ƒ
        ranges = [
            (0, 100, "å¾ˆå° (0-100)"),
            (101, 1000, "å° (101-1K)"),
            (1001, 10000, "ä¸­ (1K-10K)"),
            (10001, 100000, "å¤§ (10K-100K)"),
            (100001, float("inf"), "å¾ˆå¤§ (>100K)"),
        ]

        print(f"   å¤§å°åˆ†å¸ƒ:")
        for min_size, max_size, label in ranges:
            count = sum(1 for s in sizes if min_size <= s <= max_size)
            if count > 0:
                percentage = (count / len(sizes)) * 100
                print(f"     {label:15} {count:6} ä¸ªæ–‡ä»¶ ({percentage:5.1f}%)")

    print("=" * 50 + "\n")


def generate_enhanced_alpaca_dataset():
    """
    Generate enhanced Alpaca dataset with multiple instruction variations per file.

    Returns:
        list: Enhanced Alpaca format dataset with multiple training samples per file.
    """
    # é¦–å…ˆç”ŸæˆåŸºç¡€æ•°æ®é›†
    basic_dataset = generate_dataset()

    # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆå¤šç§instructionå˜åŒ–
    enhanced_dataset = []

    instruction_templates = {
        "analysis": [
            "è¯·è¯¦ç»†åˆ†æè¿™ä¸ªä»£ç æ–‡ä»¶çš„åŠŸèƒ½å’Œå®ç°åŸç†",
            "åˆ†æè¿™ä¸ªæ–‡ä»¶ä¸­çš„æ ¸å¿ƒç®—æ³•å’Œæ•°æ®ç»“æ„",
            "è¯·è§£é‡Šè¿™ä¸ªä»£ç çš„è®¾è®¡æ¨¡å¼å’Œæ¶æ„æ€è·¯",
        ],
        "explanation": [
            "è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šè¿™æ®µä»£ç çš„ä½œç”¨",
            "è¿™ä¸ªä»£ç æ–‡ä»¶å®ç°äº†ä»€ä¹ˆåŠŸèƒ½ï¼Ÿè¯·è¯¦ç»†è¯´æ˜",
            "è¯·é€è¡Œè§£é‡Šè¿™ä¸ªä»£ç çš„æ‰§è¡Œé€»è¾‘",
        ],
        "optimization": [
            "è¯·è¯„ä¼°è¿™ä¸ªä»£ç çš„æ€§èƒ½å¹¶æå‡ºä¼˜åŒ–å»ºè®®",
            "è¿™ä¸ªä»£ç æœ‰å“ªäº›å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼Ÿ",
            "ä»ä»£ç è´¨é‡è§’åº¦åˆ†æè¿™ä¸ªæ–‡ä»¶çš„ä¼˜ç¼ºç‚¹",
        ],
        "documentation": [
            "è¯·ä¸ºè¿™ä¸ªä»£ç æ–‡ä»¶ç”Ÿæˆè¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£",
            "å¦‚ä½•ä¸ºè¿™ä¸ªä»£ç ç¼–å†™å•å…ƒæµ‹è¯•ï¼Ÿ",
            "è¯·æ€»ç»“è¿™ä¸ªæ–‡ä»¶çš„APIæ¥å£å’Œä½¿ç”¨æ–¹æ³•",
        ],
    }

    for item in basic_dataset:
        # ä¿ç•™åŸå§‹æ¡ç›®
        enhanced_dataset.append(item)

        # ä¸ºæ¯ç§æ¨¡æ¿ç”Ÿæˆé¢å¤–çš„è®­ç»ƒæ ·æœ¬
        for template_type, templates in instruction_templates.items():
            for template in templates:
                enhanced_entry = item.copy()
                file_path = item["input"].split("æ–‡ä»¶è·¯å¾„:")[1].split("\n")[0].strip()
                enhanced_entry["instruction"] = f"{template}ï¼š{file_path}"

                # æ ¹æ®ä¸åŒçš„instructionç±»å‹è°ƒæ•´system prompt
                if template_type == "optimization":
                    enhanced_entry["system"] = (
                        "ä½ æ˜¯ä¸€ä¸ªä»£ç ä¼˜åŒ–ä¸“å®¶ï¼Œèƒ½å¤Ÿè¯†åˆ«æ€§èƒ½ç“¶é¢ˆå¹¶æä¾›å…·ä½“çš„ä¼˜åŒ–å»ºè®®ã€‚"
                    )
                elif template_type == "documentation":
                    enhanced_entry["system"] = (
                        "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ä¸“å®¶ï¼Œèƒ½å¤Ÿç¼–å†™æ¸…æ™°ã€å‡†ç¡®çš„æŠ€æœ¯æ–‡æ¡£å’Œæµ‹è¯•ç”¨ä¾‹ã€‚"
                    )
                elif template_type == "explanation":
                    enhanced_entry["system"] = (
                        "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹å¯¼å¸ˆï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚çš„ä»£ç é€»è¾‘ã€‚"
                    )

                enhanced_dataset.append(enhanced_entry)

    # ä¿å­˜å¢å¼ºç‰ˆæ•°æ®é›†
    output_file = OUTPUT_DATASET.replace(".jsonl", "_alpaca_enhanced.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(enhanced_dataset, f, ensure_ascii=False, indent=2)

    print(
        f"ğŸš€ å¢å¼ºç‰ˆAlpacaæ•°æ®é›†ç”Ÿæˆå®Œæˆ: {output_file} (å…± {len(enhanced_dataset)} æ¡è®­ç»ƒæ ·æœ¬)"
    )
    return enhanced_dataset


# =============================
# ä¸»ç¨‹åº
# =============================
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # æ­¥éª¤1ï¼šç”Ÿæˆæ•°æ®é›†
    generate_dataset()
