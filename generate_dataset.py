# train_your_code_model.py
import json
import hashlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"

import os
import logging

# =============================
# é…ç½®åŒºï¼ˆæ ¹æ®ä½ çš„çŽ¯å¢ƒä¿®æ”¹ï¼‰
# =============================
REPO_ROOT = "."
OUTPUT_DATASET = "full_text_dataset.json"
TRAINED_MODEL_OUTPUT = "qwen-coder-finetuned"
MODEL_NAME = "Qwen/Qwen2.5-Coder-1.5B"

# äºŒè¿›åˆ¶æ–‡ä»¶é»‘åå•ï¼ˆåªè¿™äº›æ‰æŽ’é™¤ï¼‰
BINARY_EXTENSIONS = {
    ".jpg",
    ".jpeg",
    ".png",
    ".gif",
    ".bmp",
    ".tiff",
    ".ico",
    ".svg",
    ".webp",
    ".mp3",
    ".wav",
    ".flac",
    ".aac",
    ".ogg",
    ".mp4",
    ".avi",
    ".mov",
    ".wmv",
    ".webm",
    ".zip",
    ".tar",
    ".gz",
    ".tgz",
    ".rar",
    ".7z",
    ".bz2",
    ".xz",
    ".iso",
    ".dmg",
    ".exe",
    ".dll",
    ".so",
    ".dylib",
    ".bin",
    ".out",
    ".class",
    ".jar",
    ".apk",
    ".ipa",
    ".pdb",
    ".o",
    ".obj",
    ".lib",
    ".a",
    ".db",
    ".sqlite",
    ".mdb",
    ".accdb",
    ".doc",
    ".docx",
    ".xls",
    ".xlsx",
    ".ppt",
    ".pptx",
    ".pdf",
    ".psd",
    ".ai",
    ".indd",
    ".sketch",
    ".dwg",
    ".blend",
}

# æŽ’é™¤çš„ç›®å½•ï¼ˆä¾èµ–ã€ç¼“å­˜ã€ä¸´æ—¶ï¼‰
EXCLUDE_DIRS = {
    ".git",
    "node_modules",
    "__pycache__",
    "venv",
    ".venv",
    "env",
    "dist",
    "build",
    "coverage",
    "logs",
    "log",
    "tmp",
    "temp",
    "cache",
    ".idea",
    ".vscode",
    ".pytest_cache",
    ".mypy_cache",
    ".tox",
    "examples",
    "scripts",
    "tests",
    "test",
    "testing",
    "docs",
    "doc",
    "public",
    "assets",
    "images",
    "img",
    "uploads",
    "download",
}

# æŽ’é™¤çš„æ–‡ä»¶å
EXCLUDE_FILES = {
    "ai.py",
    "generate_dataset.py",
    "clone_repos.sh",
    "train_your_code_model.py",
    "package-lock.json",
    "yarn.lock",
    "pnpm-lock.yaml",
    ".gitignore",
    ".dockerignore",
    ".prettierignore",
    "README.md",
    "readme.md",
    "Readme.md",
    "LICENSE",
    "license.txt",
    "requirements.txt",
    "setup.py",
    "pyproject.toml",
    "Dockerfile",
    "docker-compose.yml",
    "Makefile",
}

# æŽ’é™¤çš„é€šé…æ¨¡å¼
EXCLUDE_PATTERNS = [
    "*.log",
    "*.tmp",
    "*~",
    ".*.sw*",
    "#*#",
    "*.pid",
    "*.min.js",
    "*.min.css",
    "*.bundle.js",
    "*.lock",
]


# å¤šçº¿ç¨‹
NUM_WORKERS = None

# è¯­è¨€æ˜ å°„
EXT_TO_LANGUAGE = {
    ".py": "python",
    ".js": "javascript",
    ".jsx": "react",
    ".ts": "typescript",
    ".tsx": "typescript-react",
    ".html": "html",
    ".css": "css",
    ".sh": "shell",
    ".go": "go",
    ".rs": "rust",
    ".java": "java",
    ".cpp": "cpp",
    ".c": "c",
    ".json": "json",
    ".yaml": "yaml",
    ".yml": "yaml",
    ".md": "markdown",
    ".txt": "text",
    ".sql": "sql",
    ".tf": "terraform",
    ".ipynb": "jupyter",
    ".dockerfile": "dockerfile",
    ".xml": "xml",
    ".toml": "toml",
    ".php": "php",
}

# è®­ç»ƒå‚æ•°
MAX_SEQ_LENGTH = 1024
BATCH_SIZE = 8
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 1e-4
NUM_EPOCHS = 3
SAVE_STEPS = 50
LOG_STEPS = 10

# LoRA å‚æ•°
LORA_R = 128
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj",
]


# =============================
# åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡ä»¶
# =============================
def is_text_file(file_path: Path, sample_size: int = 1024) -> bool:
    """
    Checks if a file is a text file by sampling its content.

    Args:
        - `file_path`: Path to the file.
        - `sample_size`: Number of bytes to sample from the file.

    Returns:
        True if the file is likely a text file, False otherwise.
    """
    try:
        with open(file_path, "rb") as f:
            sample = f.read(sample_size)
            if not sample:
                return False
            nontext_ratio = sum(
                1 for c in sample if c < 0x20 and c not in (9, 10, 13)
            ) / len(sample)
            return nontext_ratio < 0.3
    except Exception:
        return False


# =============================
# å¤„ç†å•ä¸ªæ–‡ä»¶
# =============================
def process_file(file_path: Path, repo_root: Path):
    """
    Process a single file to extract content and metadata.

    Args:
        file_path (Path): Path to the file.
        repo_root (Path): Path to the repository root.

    Returns:
        dict | None: A dictionary with the following keys if processed successfully:
            - `text` (str): File content.
            - `file_path` (str): Relative path from repo root.
            - `language` (str): Detected language based on extension.
            - `size` (int): Length of the text content.
        Returns None if the file is excluded or cannot be processed.
    """
    try:
        ext = file_path.suffix.lower()
        if ext in BINARY_EXTENSIONS:
            return None
        if any(part.lower() in EXCLUDE_DIRS for part in file_path.parts):
            return None
        if file_path.name.lower() in EXCLUDE_FILES:
            return None
        if any(file_path.match(p) for p in EXCLUDE_PATTERNS):
            return None
        if file_path.stat().st_size == 0:
            return None
        if not is_text_file(file_path):
            return None

        encodings = ["utf-8", "utf-8-sig", "latin1", "cp1252", "gbk"]
        content = None
        for enc in encodings:
            try:
                with open(file_path, "r", encoding=enc, errors="ignore") as f:
                    content = f.read().strip()
                break
            except Exception:
                continue

        return {
            "text": content,
            "file_path": str(file_path.relative_to(repo_root)),
            "language": EXT_TO_LANGUAGE.get(ext, ext.replace(".", "") or "unknown"),
            "size": len(content),
        }
    except Exception:
        return None


def create_alpaca_entry(item):
    """
    Create an Alpaca format entry from processed file item.

    Args:
        item (dict): Processed file item with text, file_path, language, size

    Returns:
        dict: Alpaca format entry
    """
    file_path = item["file_path"]
    language = item["language"]
    content = item["text"]

    # æ ¹æ®æ–‡ä»¶ç±»åž‹ç”Ÿæˆä¸åŒçš„instruction
    if language in ["python", "py"]:
        instruction = f"è¯·è§£é‡Šè¿™ä¸ªPythonä»£ç æ–‡ä»¶çš„åŠŸèƒ½å’Œå®žçŽ°é€»è¾‘"
        system = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonä»£ç åˆ†æžå¸ˆï¼Œèƒ½å¤Ÿè¯¦ç»†è§£é‡Šä»£ç çš„åŠŸèƒ½ã€ç»“æž„å’Œå®žçŽ°ç»†èŠ‚ã€‚"
        )
    elif language in ["javascript", "js", "ts", "typescript"]:
        instruction = f"è¯·åˆ†æžè¿™ä¸ªJavaScript/TypeScriptä»£ç çš„åŠŸèƒ½å’Œè®¾è®¡æ¨¡å¼"
        system = "ä½ æ˜¯ä¸€ä¸ªå‰ç«¯å¼€å‘ä¸“å®¶ï¼Œæ“…é•¿åˆ†æžJavaScriptå’ŒTypeScriptä»£ç çš„è®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®žè·µã€‚"
    elif language in ["java"]:
        instruction = f"è¯·è§£æžè¿™ä¸ªJavaä»£ç çš„ç»“æž„å’ŒåŠŸèƒ½å®žçŽ°"
        system = "ä½ æ˜¯ä¸€ä¸ªJavaå¼€å‘ä¸“å®¶ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æžJavaä»£ç çš„é¢å‘å¯¹è±¡è®¾è®¡å’ŒåŠŸèƒ½å®žçŽ°ã€‚"
    elif language in ["cpp", "c++", "c"]:
        instruction = f"è¯·åˆ†æžè¿™ä¸ªC/C++ä»£ç çš„ç®—æ³•å’Œæ•°æ®ç»“æž„å®žçŽ°"
        system = "ä½ æ˜¯ä¸€ä¸ªç³»ç»Ÿçº§ç¼–ç¨‹ä¸“å®¶ï¼Œç²¾é€šC/C++çš„å†…å­˜ç®¡ç†ã€ç®—æ³•ä¼˜åŒ–å’Œç³»ç»Ÿè®¾è®¡ã€‚"
    elif language in ["html"]:
        instruction = f"è¯·åˆ†æžè¿™ä¸ªHTMLæ–‡ä»¶çš„ç»“æž„å’Œè¯­ä¹‰"
        system = "ä½ æ˜¯ä¸€ä¸ªå‰ç«¯å¼€å‘ä¸“å®¶ï¼Œæ“…é•¿HTMLè¯­ä¹‰åŒ–å’ŒWebæ ‡å‡†ã€‚"
    elif language in ["css"]:
        instruction = f"è¯·è§£é‡Šè¿™ä¸ªCSSæ ·å¼æ–‡ä»¶çš„è®¾è®¡æ€è·¯å’Œå¸ƒå±€æ–¹æ¡ˆ"
        system = "ä½ æ˜¯ä¸€ä¸ªUI/UXè®¾è®¡å¸ˆï¼Œç²¾é€šCSSå¸ƒå±€ã€åŠ¨ç”»å’Œå“åº”å¼è®¾è®¡ã€‚"
    elif language in ["markdown", "md"]:
        instruction = f"è¯·æ€»ç»“è¿™ä¸ªMarkdownæ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œç»“æž„"
        system = "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ä¸“å®¶ï¼Œèƒ½å¤Ÿå‡†ç¡®æå–å’Œæ€»ç»“æ–‡æ¡£çš„æ ¸å¿ƒä¿¡æ¯ã€‚"
    elif language in ["json"]:
        instruction = f"è¯·è§£é‡Šè¿™ä¸ªJSONé…ç½®æ–‡ä»¶çš„ç»“æž„å’Œç”¨é€”"
        system = "ä½ æ˜¯ä¸€ä¸ªç³»ç»Ÿé…ç½®ä¸“å®¶ï¼Œèƒ½å¤Ÿè§£é‡Šå„ç§é…ç½®æ–‡ä»¶çš„ä½œç”¨å’Œæœ€ä½³å®žè·µã€‚"
    elif language in ["yaml", "yml"]:
        instruction = f"è¯·åˆ†æžè¿™ä¸ªYAMLé…ç½®æ–‡ä»¶çš„é…ç½®é¡¹å’Œç”¨é€”"
        system = "ä½ æ˜¯ä¸€ä¸ªDevOpså·¥ç¨‹å¸ˆï¼Œç²¾é€šå„ç§é…ç½®æ–‡ä»¶æ ¼å¼å’Œéƒ¨ç½²é…ç½®ã€‚"
    else:
        instruction = f"è¯·åˆ†æžè¿™ä¸ª{language}æ–‡ä»¶çš„å†…å®¹å’ŒåŠŸèƒ½"
        system = f"ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„{language}å¼€å‘ä¸“å®¶ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æžä»£ç ç»“æž„å’Œå®žçŽ°é€»è¾‘ã€‚"

    # ç”Ÿæˆé’ˆå¯¹å…·ä½“æ–‡ä»¶çš„instruction
    instruction = f"{instruction}ï¼š{file_path}"

    return {
        "instruction": instruction,
        "input": f"æ–‡ä»¶è·¯å¾„: {file_path}\nè¯­è¨€ç±»åž‹: {language}\næ–‡ä»¶å¤§å°: {item['size']} å­—ç¬¦\n\næ–‡ä»¶å†…å®¹:\n```{language}\n{content}\n```",
        "output": f"è¿™æ˜¯ä¸€ä¸ª{language}æ–‡ä»¶ï¼Œä½äºŽ `{file_path}`ã€‚æ–‡ä»¶åŒ…å« {item['size']} ä¸ªå­—ç¬¦çš„ä»£ç å†…å®¹ã€‚\n\nåŸºäºŽæ–‡ä»¶å†…å®¹çš„åˆ†æžï¼Œè¯¥æ–‡ä»¶ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\n\n1. **æ–‡ä»¶ç»“æž„**: è¯¥æ–‡ä»¶é‡‡ç”¨äº†æ ‡å‡†çš„{language}è¯­æ³•ç»“æž„\n2. **ä¸»è¦åŠŸèƒ½**: éœ€è¦æ ¹æ®å…·ä½“ä»£ç å†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æž\n3. **æŠ€æœ¯ç‰¹ç‚¹**: ä½¿ç”¨äº†{language}çš„ç›¸å…³ç‰¹æ€§å’Œæœ€ä½³å®žè·µ\n4. **ä»£ç è´¨é‡**: ä»£ç ç»“æž„æ¸…æ™°ï¼Œç¬¦åˆ{language}çš„ç¼–ç è§„èŒƒ\n\nå»ºè®®è¿›ä¸€æ­¥åˆ†æžå…·ä½“çš„å‡½æ•°ã€ç±»æˆ–æ¨¡å—å®žçŽ°æ¥äº†è§£æ›´è¯¦ç»†çš„åŠŸèƒ½é€»è¾‘ã€‚",
        "system": system,
        "history": [],
    }


# =============================
# ç”Ÿæˆæ•°æ®é›†
# =============================
def generate_dataset():
    """
    Generate an Alpaca format dataset from repository files.

    Args:
        None

    Returns:
        list: A list of dictionaries in Alpaca format with the following keys:
            - `instruction` (str): User instruction or question.
            - `input` (str): Context information including file path, language, and content.
            - `output` (str): Model response analyzing the code.
            - `system` (str): System prompt or role setting.
            - `history` (list): Historical dialogue, empty for new conversations.
    """
    repo_path = Path(REPO_ROOT).resolve()
    print(f"ðŸ” æ‰«æä»“åº“: {repo_path}")

    all_files = [f for f in repo_path.rglob("*") if f.is_file()]
    print(f"âœ… å‘çŽ° {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    # å¤„ç†æ–‡ä»¶å†…å®¹
    results = []
    with ThreadPoolExecutor(
        max_workers=NUM_WORKERS or (os.cpu_count() or 1) * 2
    ) as exec:
        futures = {exec.submit(process_file, fp, repo_path): fp for fp in all_files}
        for future in as_completed(futures):
            item = future.result()
            if item:
                results.append(item)

    # åŽ»é‡
    seen = set()
    unique = []
    for item in results:
        h = hashlib.md5(item["text"].encode("utf-8")).hexdigest()
        if h not in seen:
            seen.add(h)
            unique.append(item)

    # è½¬æ¢ä¸ºAlpacaæ ¼å¼
    print(f"ðŸ”„ è½¬æ¢ä¸ºAlpacaæ ¼å¼...")
    alpaca_dataset = []
    for item in unique:
        try:
            alpaca_entry = create_alpaca_entry(item)
            alpaca_dataset.append(alpaca_entry)
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {item.get('file_path', 'unknown')} æ—¶å‡ºé”™: {e}")
            continue

    # ä¿å­˜ä¸ºæ ‡å‡†JSONæ ¼å¼
    with open(OUTPUT_DATASET, "w", encoding="utf-8") as f:
        json.dump(alpaca_dataset, f, ensure_ascii=False, indent=2)

    print(
        f"ðŸŽ‰ Alpacaæ ¼å¼æ•°æ®é›†ç”Ÿæˆå®Œæˆ: {OUTPUT_DATASET} (å…± {len(alpaca_dataset)} æ¡è®­ç»ƒæ ·æœ¬)"
    )

    # è¾“å‡ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    language_stats = {}
    for entry in alpaca_dataset:
        # ä»Žinputä¸­æå–è¯­è¨€ä¿¡æ¯
        input_text = entry["input"]
        if "è¯­è¨€ç±»åž‹:" in input_text:
            lang = input_text.split("è¯­è¨€ç±»åž‹:")[1].split("\n")[0].strip()
            language_stats[lang] = language_stats.get(lang, 0) + 1

    print(f"\nðŸ“Š æ•°æ®é›†è¯­è¨€åˆ†å¸ƒ:")
    for lang, count in sorted(language_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {lang}: {count} ä¸ªæ–‡ä»¶")

    return alpaca_dataset


def generate_enhanced_alpaca_dataset():
    """
    Generate enhanced Alpaca dataset with multiple instruction variations per file.

    Returns:
        list: Enhanced Alpaca format dataset with multiple training samples per file.
    """
    # é¦–å…ˆç”ŸæˆåŸºç¡€æ•°æ®é›†
    basic_dataset = generate_dataset()

    # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆå¤šç§instructionå˜åŒ–
    enhanced_dataset = []

    instruction_templates = {
        "analysis": [
            "è¯·è¯¦ç»†åˆ†æžè¿™ä¸ªä»£ç æ–‡ä»¶çš„åŠŸèƒ½å’Œå®žçŽ°åŽŸç†",
            "åˆ†æžè¿™ä¸ªæ–‡ä»¶ä¸­çš„æ ¸å¿ƒç®—æ³•å’Œæ•°æ®ç»“æž„",
            "è¯·è§£é‡Šè¿™ä¸ªä»£ç çš„è®¾è®¡æ¨¡å¼å’Œæž¶æž„æ€è·¯",
        ],
        "explanation": [
            "è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šè¿™æ®µä»£ç çš„ä½œç”¨",
            "è¿™ä¸ªä»£ç æ–‡ä»¶å®žçŽ°äº†ä»€ä¹ˆåŠŸèƒ½ï¼Ÿè¯·è¯¦ç»†è¯´æ˜Ž",
            "è¯·é€è¡Œè§£é‡Šè¿™ä¸ªä»£ç çš„æ‰§è¡Œé€»è¾‘",
        ],
        "optimization": [
            "è¯·è¯„ä¼°è¿™ä¸ªä»£ç çš„æ€§èƒ½å¹¶æå‡ºä¼˜åŒ–å»ºè®®",
            "è¿™ä¸ªä»£ç æœ‰å“ªäº›å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼Ÿ",
            "ä»Žä»£ç è´¨é‡è§’åº¦åˆ†æžè¿™ä¸ªæ–‡ä»¶çš„ä¼˜ç¼ºç‚¹",
        ],
        "documentation": [
            "è¯·ä¸ºè¿™ä¸ªä»£ç æ–‡ä»¶ç”Ÿæˆè¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£",
            "å¦‚ä½•ä¸ºè¿™ä¸ªä»£ç ç¼–å†™å•å…ƒæµ‹è¯•ï¼Ÿ",
            "è¯·æ€»ç»“è¿™ä¸ªæ–‡ä»¶çš„APIæŽ¥å£å’Œä½¿ç”¨æ–¹æ³•",
        ],
    }

    for item in basic_dataset:
        # ä¿ç•™åŽŸå§‹æ¡ç›®
        enhanced_dataset.append(item)

        # ä¸ºæ¯ç§æ¨¡æ¿ç”Ÿæˆé¢å¤–çš„è®­ç»ƒæ ·æœ¬
        for template_type, templates in instruction_templates.items():
            for template in templates:
                enhanced_entry = item.copy()
                file_path = item["input"].split("æ–‡ä»¶è·¯å¾„:")[1].split("\n")[0].strip()
                enhanced_entry["instruction"] = f"{template}ï¼š{file_path}"

                # æ ¹æ®ä¸åŒçš„instructionç±»åž‹è°ƒæ•´system prompt
                if template_type == "optimization":
                    enhanced_entry["system"] = (
                        "ä½ æ˜¯ä¸€ä¸ªä»£ç ä¼˜åŒ–ä¸“å®¶ï¼Œèƒ½å¤Ÿè¯†åˆ«æ€§èƒ½ç“¶é¢ˆå¹¶æä¾›å…·ä½“çš„ä¼˜åŒ–å»ºè®®ã€‚"
                    )
                elif template_type == "documentation":
                    enhanced_entry["system"] = (
                        "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ä¸“å®¶ï¼Œèƒ½å¤Ÿç¼–å†™æ¸…æ™°ã€å‡†ç¡®çš„æŠ€æœ¯æ–‡æ¡£å’Œæµ‹è¯•ç”¨ä¾‹ã€‚"
                    )
                elif template_type == "explanation":
                    enhanced_entry["system"] = (
                        "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹å¯¼å¸ˆï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚çš„ä»£ç é€»è¾‘ã€‚"
                    )

                enhanced_dataset.append(enhanced_entry)

    # ä¿å­˜å¢žå¼ºç‰ˆæ•°æ®é›†
    with open(OUTPUT_DATASET, "w", encoding="utf-8") as f:
        json.dump(enhanced_dataset, f, ensure_ascii=False, indent=2)

    print(
        f"ðŸš€ å¢žå¼ºç‰ˆAlpacaæ•°æ®é›†ç”Ÿæˆå®Œæˆ: {OUTPUT_DATASET} (å…± {len(enhanced_dataset)} æ¡è®­ç»ƒæ ·æœ¬)"
    )
    return enhanced_dataset


# =============================
# ä¸»ç¨‹åº
# =============================
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # æ­¥éª¤1ï¼šç”Ÿæˆæ•°æ®é›†
    generate_dataset()
