import os
import json
import hashlib
import mimetypes
from datetime import datetime
from pathlib import Path
import base64
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import time


class ThreadSafeFileProcessor:
    def __init__(self, source_dir="source", output_file="dataset.md", max_workers=None):
        self.source_dir = source_dir
        self.output_file = output_file
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)

        # çº¿ç¨‹å®‰å…¨çš„æ•°æ®ç»“æ„
        self.dataset = []
        self.dataset_lock = threading.Lock()
        self.progress_lock = threading.Lock()

        # è¿›åº¦è·Ÿè¸ª
        self.processed_count = 0
        self.total_files = 0
        self.start_time = None

        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        self.text_extensions = {
            ".txt",
            ".md",
            ".py",
            ".js",
            ".html",
            ".css",
            ".json",
            ".xml",
            ".csv",
            ".sql",
            ".yml",
            ".yaml",
            ".ini",
            ".cfg",
            ".conf",
            ".java",
            ".cpp",
            ".c",
            ".h",
            ".php",
            ".rb",
            ".go",
            ".rs",
            ".sh",
            ".bat",
            ".ps1",
            ".dockerfile",
            ".gitignore",
            ".env",
            ".log",
            ".readme",
            ".license",
            ".makefile",
            ".cmake",
        }

        self.image_extensions = {
            ".jpg",
            ".jpeg",
            ".png",
            ".gif",
            ".bmp",
            ".tiff",
            ".webp",
            ".svg",
        }

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "files_per_second": 0,
            "total_processing_time": 0,
            "thread_stats": {},
        }

    def get_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ - ä¼˜åŒ–ç‰ˆæœ¬"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            return f"error: {str(e)}"

    def read_text_file(self, file_path):
        """è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹ - ä¼˜åŒ–ç‰ˆæœ¬"""
        encodings = ["utf-8", "gbk", "gb2312", "latin1"]
        for encoding in encodings:
            try:
                with open(file_path, "r", encoding=encoding, buffering=8192) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
            except Exception as e:
                return f"è¯»å–é”™è¯¯: {str(e)}"
        return "æ— æ³•è§£ç æ–‡ä»¶å†…å®¹"

    def encode_binary_file(self, file_path):
        """å°†äºŒè¿›åˆ¶æ–‡ä»¶ç¼–ç ä¸ºbase64 - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            with open(file_path, "rb") as f:
                file_size = os.path.getsize(file_path)
                if file_size > 10 * 1024 * 1024:  # å¤§äº10MB
                    return "æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡ç¼–ç "
                return base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            return f"ç¼–ç é”™è¯¯: {str(e)}"

    def get_file_info(self, file_path):
        """è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
        try:
            stat = os.stat(file_path)
            return {
                "size": stat.st_size,
                "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "mime_type": mimetypes.guess_type(file_path)[0] or "unknown",
            }
        except Exception as e:
            return {"size": 0, "error": str(e)}

    def process_single_file(self, file_path, file_id):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        thread_id = threading.current_thread().ident

        if thread_id not in self.stats["thread_stats"]:
            self.stats["thread_stats"][thread_id] = {"processed": 0, "errors": 0}

        try:
            file_path = Path(file_path)
            relative_path = file_path.relative_to(self.source_dir)
            extension = file_path.suffix.lower()

            file_info = self.get_file_info(file_path)

            file_data = {
                "id": file_id,
                "filename": file_path.name,
                "path": str(relative_path),
                "full_path": str(file_path),
                "extension": extension,
                "hash": self.get_file_hash(file_path),
                "file_info": file_info,
                "processed_time": datetime.now().isoformat(),
                "thread_id": thread_id,
            }

            if extension in self.text_extensions:
                file_data["type"] = "text"
                content = self.read_text_file(file_path)
                file_data["content"] = content
                file_data["encoding"] = "utf-8"
                file_data["preview"] = (
                    content[:500] + "..." if len(content) > 500 else content
                )

            elif extension in self.image_extensions:
                file_data["type"] = "image"
                size = file_info.get("size", 0)
                if size < 5 * 1024 * 1024:
                    file_data["content"] = self.encode_binary_file(file_path)
                    file_data["encoding"] = "base64"
                else:
                    file_data["content"] = "å›¾åƒæ–‡ä»¶è¿‡å¤§ï¼Œä»…ä¿å­˜å…ƒæ•°æ®"
                    file_data["encoding"] = "none"
                file_data["preview"] = f"ğŸ“· å›¾åƒæ–‡ä»¶ ({size} B)"

            else:
                file_data["type"] = "binary"
                size = file_info.get("size", 0)
                if size < 1024 * 1024:
                    file_data["content"] = self.encode_binary_file(file_path)
                    file_data["encoding"] = "base64"
                else:
                    file_data["content"] = "æ–‡ä»¶è¿‡å¤§ï¼Œä»…ä¿å­˜å…ƒæ•°æ®"
                    file_data["encoding"] = "none"
                file_data["preview"] = f"ğŸ“ äºŒè¿›åˆ¶æ–‡ä»¶ ({size} B)"

            self.stats["thread_stats"][thread_id]["processed"] += 1
            return file_data

        except Exception as e:
            self.stats["thread_stats"][thread_id]["errors"] += 1
            return {
                "id": file_id,
                "filename": (
                    file_path.name if hasattr(file_path, "name") else str(file_path)
                ),
                "path": str(file_path),
                "error": str(e),
                "processed_time": datetime.now().isoformat(),
                "thread_id": thread_id,
            }

    def update_progress(self, file_path=""):
        """æ›´æ–°è¿›åº¦ - çº¿ç¨‹å®‰å…¨"""
        with self.progress_lock:
            self.processed_count += 1
            if (
                self.processed_count % 10 == 0
                or self.processed_count == self.total_files
            ):
                elapsed = time.time() - self.start_time
                fps = self.processed_count / elapsed if elapsed > 0 else 0
                progress = (self.processed_count / self.total_files) * 100

                print(
                    f"\rè¿›åº¦: {self.processed_count}/{self.total_files} "
                    f"({progress:.1f}%) - {fps:.1f} files/s - "
                    f"ç”¨æ—¶: {elapsed:.1f}s",
                    end="",
                    flush=True,
                )

    def collect_all_files(self):
        """æ”¶é›†æ‰€æœ‰æ–‡ä»¶è·¯å¾„"""
        all_files = []
        if not os.path.exists(self.source_dir):
            print(f"é”™è¯¯: æºç›®å½• '{self.source_dir}' ä¸å­˜åœ¨")
            return all_files

        print(f"æ­£åœ¨æ‰«æç›®å½•: {self.source_dir}")
        for root, dirs, files in os.walk(self.source_dir):
            for file in files:
                file_path = os.path.join(root, file)
                all_files.append(file_path)

        self.total_files = len(all_files)
        print(f"å‘ç° {self.total_files} ä¸ªæ–‡ä»¶")
        return all_files

    def process_directory_multithread(self):
        """å¤šçº¿ç¨‹å¤„ç†ç›®å½•"""
        all_files = self.collect_all_files()
        if not all_files:
            return False

        print(f"å¼€å§‹å¤šçº¿ç¨‹å¤„ç† (å·¥ä½œçº¿ç¨‹æ•°: {self.max_workers})")
        print("-" * 60)

        self.start_time = time.time()

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {
                executor.submit(self.process_single_file, file_path, idx + 1): file_path
                for idx, file_path in enumerate(all_files)
            }

            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    with self.dataset_lock:
                        self.dataset.append(result)
                    self.update_progress(file_path)
                except Exception as e:
                    print(f"\nå¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        print("\n" + "=" * 60)
        total_time = time.time() - self.start_time
        self.stats["total_processing_time"] = total_time
        self.stats["files_per_second"] = (
            self.total_files / total_time if total_time > 0 else 0
        )

        print(f"å¤„ç†å®Œæˆ!")
        print(f"æ€»ç”¨æ—¶: {total_time:.2f} ç§’")
        print(f"å¤„ç†é€Ÿåº¦: {self.stats['files_per_second']:.2f} æ–‡ä»¶/ç§’")
        print(f"æ´»è·ƒçº¿ç¨‹æ•°: {len(self.stats['thread_stats'])}")

        return True

    def generate_markdown_report(self):
        """ç”Ÿæˆå®Œæ•´çš„ Markdown æŠ¥å‘Š"""
        total_files = len(self.dataset)
        errors = sum(1 for f in self.dataset if "error" in f)
        success = total_files - errors

        file_types = {}
        extensions = {}
        total_size = 0

        for item in self.dataset:
            if "error" in item:
                continue
            ftype = item.get("type", "unknown")
            ext = item.get("extension", "æœªçŸ¥")
            file_types[ftype] = file_types.get(ftype, 0) + 1
            extensions[ext] = extensions.get(ext, 0) + 1
            if "file_info" in item and "size" in item["file_info"]:
                total_size += item["file_info"]["size"]

        def human_readable_size(size):
            for unit in ["B", "KB", "MB", "GB"]:
                if size < 1024.0:
                    return f"{size:.2f} {unit}"
                size /= 1024.0
            return f"{size:.2f} TB"

        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        Path(self.output_file).parent.mkdir(parents=True, exist_ok=True)

        with open(self.output_file, "w", encoding="utf-8") as md:
            md.write(f"# ğŸ“ AI æ•°æ®é›†æŠ¥å‘Š\n\n")
            md.write(
                f"> ç”Ÿæˆæ—¶é—´: `{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}`\n\n"
            )

            md.write("## ğŸ“Š æ‘˜è¦ä¿¡æ¯\n\n")
            md.write("| é¡¹ç›® | å€¼ |\n")
            md.write("|------|-----|\n")
            md.write(f"| æ€»æ–‡ä»¶æ•° | {total_files} |\n")
            md.write(f"| æˆåŠŸå¤„ç† | {success} |\n")
            md.write(f"| å¤„ç†å¤±è´¥ | {errors} |\n")
            md.write(f"| æ€»å¤§å° | {human_readable_size(total_size)} |\n")
            md.write(f"| å¤„ç†è€—æ—¶ | {self.stats['total_processing_time']:.2f} ç§’ |\n")
            md.write(f"| å¤„ç†é€Ÿåº¦ | {self.stats['files_per_second']:.2f} æ–‡ä»¶/ç§’ |\n")
            md.write(f"| çº¿ç¨‹æ•° | {self.max_workers} |\n\n")

            md.write("## ğŸ§© æ–‡ä»¶ç±»å‹åˆ†å¸ƒ\n\n")
            md.write("| ç±»å‹ | æ•°é‡ |\n")
            md.write("|------|------|\n")
            for t, c in file_types.items():
                md.write(f"| `{t}` | {c} |\n")
            md.write("\n")

            md.write("## ğŸ”§ æ‰©å±•åç»Ÿè®¡\n\n")
            md.write("| æ‰©å±•å | æ•°é‡ |\n")
            md.write("|--------|------|\n")
            for ext, cnt in sorted(extensions.items(), key=lambda x: -x[1]):
                md.write(f"| `{ext}` | {cnt} |\n")
            md.write("\n")

            md.write("## âš™ï¸ çº¿ç¨‹æ€§èƒ½ç»Ÿè®¡\n\n")
            md.write("| çº¿ç¨‹ID | å¤„ç†æ–‡ä»¶æ•° | é”™è¯¯æ•° |\n")
            md.write("|--------|-----------|--------|\n")
            for tid, stat in self.stats["thread_stats"].items():
                md.write(f"| `{tid}` | {stat['processed']} | {stat['errors']} |\n")
            md.write("\n")

            md.write("## ğŸ“„ è¯¦ç»†æ–‡ä»¶åˆ—è¡¨\n\n")
            md.write("| ID | æ–‡ä»¶å | è·¯å¾„ | ç±»å‹ | å¤§å° | ä¿®æ”¹æ—¶é—´ | é¢„è§ˆ |\n")
            md.write("|----|--------|------|------|------|----------|-------|\n")

            for item in sorted(self.dataset, key=lambda x: x.get("id", 0)):
                filename = item["filename"]
                path = item["path"]
                ftype = item.get("type", "unknown")
                preview = item.get("preview", "")
                size = item["file_info"].get("size", 0)
                mtime = item["file_info"].get("modified_time", "N/A")

                md.write(
                    f"| `{item['id']}` "
                    f"| `{filename}` "
                    f"| `{path}` "
                    f"| `{ftype}` "
                    f"| `{size:,} B` "
                    f"| `{mtime.split('T')[0]}` "
                    f"| {preview.replace('|', '\\|')} |\n"
                )

            md.write("\n")

            md.write("## ğŸ” æ–‡ä»¶å†…å®¹è¯¦æƒ…\n\n")
            for item in sorted(self.dataset, key=lambda x: x.get("id", 0)):
                if "error" in item:
                    continue

                md.write(f"### ğŸ“„ æ–‡ä»¶ #{item['id']} - `{item['filename']}`\n\n")
                md.write(f"- **è·¯å¾„**: `{item['path']}`\n")
                md.write(f"- **ç±»å‹**: `{item['type']}`\n")
                md.write(f"- **å¤§å°**: `{item['file_info']['size']:,} B`\n")
                md.write(f"- **ä¿®æ”¹æ—¶é—´**: `{item['file_info']['modified_time']}`\n")
                md.write(f"- **ç¼–ç **: `{item.get('encoding', 'N/A')}`\n\n")

                content = item.get("content", "")
                if item["type"] == "text":
                    md.write("#### å†…å®¹é¢„è§ˆ\n\n")
                    md.write("```txt\n")
                    md.write(
                        (content[:2000] + "...\n")
                        if len(content) > 2000
                        else content + "\n"
                    )
                    md.write("```\n\n")

        print(f"\nâœ… Markdown æŠ¥å‘Šå·²ç”Ÿæˆ: {self.output_file}")

    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        if self.process_directory_multithread():
            self.generate_markdown_report()
            return True
        return False


def main():
    """ä¸»å‡½æ•°"""
    source_directory = "./source"  # æºç›®å½•
    output_md = "./dataset/dataset.md"  # è¾“å‡ºä¸º .md
    max_workers = None  # è‡ªåŠ¨è®¾ç½®çº¿ç¨‹æ•°

    processor = ThreadSafeFileProcessor(
        source_dir=source_directory,
        output_file=output_md,
        max_workers=max_workers,
    )

    print("=== ğŸ“ å¤šçº¿ç¨‹AIæ•°æ®é›†ç”Ÿæˆå™¨ (Markdown è¾“å‡ºç‰ˆ) ===")
    print(f"æºç›®å½•: {source_directory}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_md}")
    print(f"æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {processor.max_workers}")
    print(f"CPUæ ¸å¿ƒæ•°: {os.cpu_count()}")
    print("=" * 60)

    success = processor.run()

    if success:
        print("\nğŸ‰ æ•°æ®é›†æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆä¸º Markdown æ–‡ä»¶ï¼")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æºç›®å½•æ˜¯å¦å­˜åœ¨ã€‚")


if __name__ == "__main__":
    main()
